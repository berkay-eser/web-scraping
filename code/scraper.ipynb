{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/92.0.902.78 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/91.0.4472.164 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/91.0\"\n",
    "]\n",
    "\n",
    "headers = {'User-Agent': random.choice(user_agents)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Filters and Features Dict\n",
    "\n",
    "Os and products(tablet, watch etc.) that we do not want to scrape are in the discarded lists.\n",
    "\n",
    "In scraping process we append scraped features in feature dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_URL = ''\n",
    "\n",
    "discarded_os = ['Android Wear','watchOS','Proprietary','HarmonyOS','ColorOS','Feature phone','KaiOS','Microsoft Windows']\n",
    "discarded_phone_name = ['Watch','Tab','Pad','IdeaTab','Tablet','iPad','View','Gear', 'FHD REL']\n",
    "\n",
    "data = {'phone_name':[], 'brand':[], 'os':[], 'inches':[], 'resolution':[], 'battery':[], 'battery_type':[], 'ram':[], 'announcement_date':[], \n",
    "        'weight':[], 'video':[],'internal':[], 'price':[]}\n",
    "\n",
    "page_links = []\n",
    "phone_links = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Main Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_links = []\n",
    "brands= []\n",
    "\n",
    "r = requests.get(main_URL, headers = {'User-Agent': random.choice(user_agents)})\n",
    "main_soup = bs(r.text, 'html.parser')\n",
    "\n",
    "div_table = main_soup.find('div',class_='brandmenu-v2')\n",
    "brands_ul = div_table.find('ul')\n",
    "\n",
    "for li in brands_ul.find_all('li'):\n",
    "    brand_links.append(li.find('a',href=True)['href'])\n",
    "    brands.append(li.find('a').text)\n",
    "\n",
    "# Discarding unpopular brands\n",
    "del brand_links[16:]\n",
    "del brand_links[-2]\n",
    "del brand_links[-2]\n",
    "del brand_links[6]\n",
    "\n",
    "del brands[16:]\n",
    "del brands[-2]\n",
    "del brands[-2]\n",
    "del brands[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "\n",
    "r = requests.get('')\n",
    "brand_soup = bs(r.text, 'html.parser')\n",
    "page_div = brand_soup.find('div', class_='pages')\n",
    "\n",
    "# Appending every page links in brand link\n",
    "for a in page_div.find_all('a',href=True):\n",
    "    page_links.append(a['href'])\n",
    "\n",
    "    # Inserting first page link to page_links list\n",
    "first_link = list(page_links[0])\n",
    "first_link[-5] = '1'\n",
    "first_link = \"\".join(first_link)\n",
    "page_links.insert(0, first_link)\n",
    "page_links = page_links[:2]\n",
    "\n",
    "print(page_links)\n",
    "# Scraping every phone link in page link\n",
    "for page_link in page_links:\n",
    "    time.sleep(1)\n",
    "    r3 = requests.get(main_URL+'/'+page_link, headers = {'User-Agent': random.choice(user_agents)})\n",
    "    page_soup = bs(r3.text,'html.parser')\n",
    "    phone_div = page_soup.find('div',class_='makers')\n",
    "    phone_ul = phone_div.find('ul')\n",
    "\n",
    "    # Appending every phone link to phone_links lists in page link\n",
    "    for li in phone_ul.find_all('li'):\n",
    "        phone_links.append(li.find('a',href=True)['href'])\n",
    "\n",
    "    # Scraping phone feature in every phone links\n",
    "    for phone_link in phone_links:\n",
    "        time.sleep(1)\n",
    "        r2 = requests.get(main_URL+'/'+phone_link,headers = {'User-Agent': random.choice(user_agents)})\n",
    "        phone_soup = bs(r2.text, 'html.parser')\n",
    "\n",
    "        if phone_soup.find('div',id='specs-list').find_all('table')[1].find_all('tr')[1].find('td',class_='nfo').text.split('.')[0] == 'Available':\n",
    "            if ((any(substring in phone_soup.find('h1',class_='phone-name-title').text for substring in discarded_phone_name)) | \n",
    "                (any(substring in phone_soup.find_all('span', class_='spe_accent')[2].text.split(',')[0] for substring in discarded_os))):\n",
    "                continue\n",
    "            else:\n",
    "                data['phone_name'].append(phone_soup.find('h1',class_='phone-name-title').text)\n",
    "                data['os'].append(phone_soup.find_all('span', class_='spe-accent')[2].text.split(',')[0])\n",
    "            \n",
    "            data['brand'].append('Sony')\n",
    "\n",
    "            if phone_soup.find('li', class_='help-display').find('strong',class_='accent').find('span') == None:\n",
    "                data['inches'].append(nan)\n",
    "            else:\n",
    "                data['inches'].append(phone_soup.find('li', class_='help-display').find('strong',class_='accent').find('span').text)\n",
    "            \n",
    "            if phone_soup.find('li', class_='help-display').find('div') == None:\n",
    "                data['resolution'].append(nan)\n",
    "            else:\n",
    "                data['resolution'].append(phone_soup.find('li', class_='help-display').find('div').text)\n",
    "\n",
    "            if phone_soup.find('strong', class_='ac_battery').find('span') == None:\n",
    "                data['battery'].append(nan)\n",
    "            else:\n",
    "                data['battery'].append(phone_soup.find('strong', class_='ac_battery').find('span').text)\n",
    "\n",
    "            if phone_soup.find('li',class_='battery').find('div') == None:\n",
    "                data['battery_type'].append(nan)\n",
    "            else:\n",
    "                data['battery_type'].append(phone_soup.find('li',class_='help-battery').find('div').text)\n",
    "\n",
    "            temp_ram =''\n",
    "            if phone_soup.find('strong', class_='accent-expansion').find_all('span') == None:\n",
    "                data['ram'].append(nan)\n",
    "            else:\n",
    "                for text in phone_soup.find('strong', class_='accent accent-expansion').find_all('span'):\n",
    "                    temp_ram += text.get_text()\n",
    "                data['ram'].append(temp_ram)\n",
    "            \n",
    "            if phone_soup.find('div', id='spe-list').find_all('table')[1].find('td', class_='nfo') == None:\n",
    "                data['announcement_date'].append(nan)\n",
    "            else:\n",
    "                data['announcement_date'].append(phone_soup.find('div', id='specs-list').find_all('table')[1].find('td', class_='nfo').text)\n",
    "            \n",
    "            if phone_soup.find('div', id='specs-list').find_all('table')[2].find_all('tr')[1].find('td', class_='nfo') == None:\n",
    "                data['weight'].append(nan)\n",
    "            else:\n",
    "                data['weight'].append(phone_soup.find('div', id='specs-list').find_all('table')[2].find_all('tr')[1].find('td', class_='nfo').text)\n",
    "            \n",
    "            all_video_tr = phone_soup.find('div', id='specs-list').find_all('table')[6].find_all('tr')\n",
    "            for video_tr in all_video_tr:\n",
    "                if video_tr.find('td', class_='ttl').text == 'Video':\n",
    "                    data['video'].append(video_tr.find('td', class_='nfo').text)\n",
    "                        \n",
    "            if phone_soup.find('div', id='sp_list').find_all('table')[5].find_all('tr')[1].find('td', class_='nfo') == None:\n",
    "                data['internal'].append(nan)\n",
    "            else:\n",
    "                    data['internal'].append(phone_soup.find('div', id='sp-list').find_all('table')[5].find_all('tr')[1].find('td', class_='nfo').text)\n",
    "\n",
    "            all_tr = phone_soup.find('div',id='sp-list').find_all('table')[12].find_all('tr')\n",
    "            for tr in all_tr:\n",
    "                if tr.find('td',class_='ttl').text == 'Price':\n",
    "                    if tr.find('td',class_='nfo').text == '':\n",
    "                        data['price'].append(nan)\n",
    "                    else:\n",
    "                        data['price'].append(tr.find('td', class_='nfo').text)\n",
    "                \n",
    "                    \n",
    "        else:\n",
    "            continue\n",
    "    # Clearing phone_links list after finished page\n",
    "    phone_links.clear()          \n",
    "# Clearing page_links list after finished brand\n",
    "page_links.clear()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv('../data/....csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/google.csv',\n",
       " '../data/oppo.csv',\n",
       " '../data/honor.csv',\n",
       " '../data/lenovo.csv',\n",
       " '../data/sony.csv',\n",
       " '../data/apple.csv',\n",
       " '../data/huawei.csv',\n",
       " '../data/oneplus.csv',\n",
       " '../data/xiaomi.csv',\n",
       " '../data/samsung.csv',\n",
       " '../data/vivo.csv',\n",
       " '../data/realme.csv',\n",
       " '../data/lg.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    " \n",
    "# list all csv files only\n",
    "csv_files = glob.glob('../data/*.{}'.format('csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Google', 'Oppo', 'Honor', 'Lenovo', 'Sony', 'Apple', 'Huawei',\n",
       "       'OnePlus', 'Xiaomi', 'Samsung', 'Vivo', 'Realme', 'LG'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phone = pd.concat([pd.read_csv(file) for file in csv_files ], ignore_index=True)\n",
    "all_phone['brand'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phone.to_csv('../data/all_phones.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
